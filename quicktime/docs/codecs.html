<TITLE>Quicktime for Linux</TITLE>

<H1>Codec information</H1>

The following concerns all the codecs built into the library.<P>


<H1>DV</H1>

DV is only supported for decoding only.  Secondly, only NTSC 25
Mbit/sec 4:1:1 DV data has ever been tested.  There are two derivatives
of DV: <B>DVC</B> and <B>DVCP</B>.  The dvcp format is used by
professional equipment and can't be supported because of cost
restrictions.<P>

<H1>IMA4</H1>

The IMA4 compressor reduces 16 bit audio data to 1/4 size, with very
good quality.  This is the preferred codec for low bandwidth audio.<P>

<H1>JPEG</H1>

and JPEG is preferred for low bandwidth video.  This format writes a
seperate JPEG photo for every frame.<P>

Using the JPEG format requires calling the following function after
quicktime_set_video.<P>

<CODE>
int quicktime_set_jpeg(quicktime_t *file, int quality, int use_float);
</CODE><P>

This takes a quality factor from 1 - 100 and a booleen flag to
determine whether floating point operations should be used to slow it
down.<P>

<H1>MJPA</H1>

MJPA stores each frame as two JPEGs interlaced.  Quicktime for Linux
lets you synthesize MJPA images suitable for playback on hardware or
decompress MJPA frames captured on hardware.  The real advantage is
that it can split compression and decompression across 2 processors,
doubling the frame rate.  To enable dualized MJPA processing call:<P>

<CODE>
quicktime_set_cpus(quicktime_t *file, int cpus);<P>
</CODE>

immediately after the <B>quicktime_init</B> call.  Cpus should contain
the number of CPUs to devote to compression.<P>

After specifying MJPA in quicktime_set_video you need to call
<B>quicktime_set_jpeg</B> as described previously.<P>

<H1>PNG</H1>

This consists of one PNG image for every frame.  Like <B>RAW</B> this
codec supports 32 bit depths.<P>

<H1>YUV2</H1> 

The human eye percieves brightness much more accurately than colors. 
YUV2 downsamples the color components by 50% for a total compression of
33% with virtually no image degredation.  This is preferred for
intermediate storage.  YUV2 is called "Component video" in Losedows and
Mac OS land.<P>

<H1>YUV4</H1>

YUV4 is planar YUV, identical to MPEG.  It downsamples the color
components by 75% for a total 50% compression.  This is the preferred
intermediate format for working with MPEG.<P>

<H1>RAW</H1>

RAW identifies both a video and an audio codec.  When you specify RAW
for an <B>audio</B> track you invoke unsigned 8 bit encoding so you'll probably
never use it.<P>

When you specify RAW for a <B>video</B> track you get RGB packed
frames.  RAW video supports alpha channels.  To get RGBA packed frames
you can then issue <P>

<CODE>
int quicktime_set_depth(quicktime_t *file, int depth, int track);
</CODE><P>

specifying a depth of 32.  In this case the input format for
<B>encode_video</B> functions is RGBA packed as well.<P>

<H1>TWOS</H1>

Twos is the preferred encoding for audio.  It stores 8, 16, and 24 bit
audio, interleaved for multiple channels.  The 8 bit mode is signed. 
The 16 and 24 bit modes are big endian signed.  There is no codec for
these formats because they're simple enough to access raw.
